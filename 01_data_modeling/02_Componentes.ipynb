{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A. Schema (modelo lÃ³gico)\n",
    "\n",
    "Define:\n",
    "- Columnas\n",
    "- Tipos de datos\n",
    "- Nullabilidad\n",
    "\n",
    "Importancia:\n",
    "- Permite optimizaciÃ³n (Catalyst)\n",
    "- Reduce errores de inferencia\n",
    "- Mejora performance\n",
    "\n",
    "Idea clave:\n",
    "ðŸ‘‰ El schema **no afecta el paralelismo**, pero sÃ­ el **plan de ejecuciÃ³n**\n"
   ],
   "id": "ea8afc0504bf7c6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Configuracion base",
   "id": "afc4f2b4fa14fa47"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:31.713028Z",
     "start_time": "2026-02-02T21:27:31.683051Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spark_labs\") \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.default.parallelism\", \"12\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Componentes:",
   "id": "2ffa1da78e49e6b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A. Schema\n",
    "Define:\n",
    "- Columnas\n",
    "- Tipos de datos\n",
    "- Nullabilidad\n",
    "\n",
    "Importancia:\n",
    "- Permite optimizaciÃ³n (Catalyst)\n",
    "- Reduce errores de inferencia\n",
    "- Mejora performance\n",
    "\n",
    "Idea clave:\n",
    "ðŸ‘‰ El schema **no afecta el paralelismo**, pero sÃ­ el **plan de ejecuciÃ³n**\n"
   ],
   "id": "e4d53f0bb8a00feb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:31.872996Z",
     "start_time": "2026-02-02T21:27:31.723140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NO REALIZAR DE ESTA FORMA\n",
    "df_bad = spark.read.csv(\"./src/ventas.csv\", header=True)\n",
    "print(\"-\"*10)\n",
    "df_bad.printSchema() # Ver el schema\n",
    "print(\"-\"*10)\n",
    "df_bad.rdd.getNumPartitions() # Ver cuÃ¡ntas particiones tiene el DataFrame"
   ],
   "id": "c3a362abadc8f11d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "root\n",
      " |-- id_venta: string (nullable = true)\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- monto: string (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      " |-- cliente_id: string (nullable = true)\n",
      " |-- producto: string (nullable = true)\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:31.900617Z",
     "start_time": "2026-02-02T21:27:31.878212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FORMA CORRECTA\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id_venta\", IntegerType(), False),\n",
    "    StructField(\"fecha\", DateType(), True),\n",
    "    StructField(\"monto\", DoubleType(), True),\n",
    "    StructField(\"pais\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\n",
    "    \"./src/ventas.csv\",\n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "print(\"-\"*10)\n",
    "df.printSchema() # Ver el schema\n",
    "print(\"-\"*10)"
   ],
   "id": "be763b3fcf358e68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "root\n",
      " |-- id_venta: integer (nullable = true)\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- monto: double (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### B. Particiones (modelo fÃ­sico)\n",
    "\n",
    "Las particiones definen:\n",
    "- CÃ³mo se **divide** el DataFrame\n",
    "- QuÃ© tareas se ejecutan en paralelo\n",
    "\n",
    "Ejemplo mental:\n",
    "- 6 cores â†’ mÃ¡ximo 6 tasks simultÃ¡neas\n",
    "- 2 particiones â†’ solo 2 cores trabajan\n",
    "\n",
    "Regla:\n",
    "ðŸ‘‰ El paralelismo depende de las particiones\n"
   ],
   "id": "bc3448d3da44d3bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "repartition(6)\n",
    "\n",
    "* Siempre redistribuye los datos\n",
    "* Siempre hace shuffle\n",
    "* Crea 6 particiones balanceadas\n",
    "* Es costoso, pero correcto para paralelismo"
   ],
   "id": "46232d93e9d5cb51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:31.937685Z",
     "start_time": "2026-02-02T21:27:31.908636Z"
    }
   },
   "cell_type": "code",
   "source": "df.rdd.getNumPartitions() # Ver cuÃ¡ntas particiones tiene el DataFrame",
   "id": "de8d4533c9a0f3b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:32.025535Z",
     "start_time": "2026-02-02T21:27:31.943502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_repart = df.repartition(6)\n",
    "df_repart.rdd.getNumPartitions()"
   ],
   "id": "fde077418df85480",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "coalesce(6)\n",
    "\n",
    "* NO redistribuye los datos\n",
    "* NO hace shuffle\n",
    "* Solo junta particiones existentes\n",
    "* Solo tiene sentido para reducir particiones"
   ],
   "id": "5a476d2aae62c847"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:32.106554Z",
     "start_time": "2026-02-02T21:27:32.042033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_small = df_repart.coalesce(3)\n",
    "df_small.rdd.getNumPartitions()"
   ],
   "id": "962dfca58ea1e394",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### C. Linaje (DAG)\n",
    "\n",
    "El linaje es:\n",
    "- El grafo de transformaciones\n",
    "- NO es ejecuciÃ³n inmediata\n",
    "\n",
    "Ejemplo:\n",
    "```text\n",
    "read â†’ filter â†’ select â†’ join â†’ groupBy"
   ],
   "id": "e8b6f8a2e92141e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:32.242468Z",
     "start_time": "2026-02-02T21:27:32.118648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_filtrado = df.filter(df.monto > 100)\n",
    "df_select = df_filtrado.select(\"pais\", \"monto\")\n",
    "df_group = df_select.groupBy(\"pais\").sum(\"monto\")\n",
    "\n",
    "df_group.show()   # â† ACCIÃ“N\n"
   ],
   "id": "c98fac334444c8da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|pais|sum(monto)|\n",
      "+----+----------+\n",
      "|  CL|     850.0|\n",
      "|  PE|    1770.5|\n",
      "|  MX|    2800.0|\n",
      "|  CO|     700.0|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:32.261287Z",
     "start_time": "2026-02-02T21:27:32.245476Z"
    }
   },
   "cell_type": "code",
   "source": "df_group.explain(True)\n",
   "id": "c6443de9df295241",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['pais], ['pais, unresolvedalias('sum(monto#243))]\n",
      "+- Project [pais#244, monto#243]\n",
      "   +- Filter (monto#243 > cast(100 as double))\n",
      "      +- Relation [id_venta#241,fecha#242,monto#243,pais#244] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "pais: string, sum(monto): double\n",
      "Aggregate [pais#244], [pais#244, sum(monto#243) AS sum(monto)#261]\n",
      "+- Project [pais#244, monto#243]\n",
      "   +- Filter (monto#243 > cast(100 as double))\n",
      "      +- Relation [id_venta#241,fecha#242,monto#243,pais#244] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [pais#244], [pais#244, sum(monto#243) AS sum(monto)#261]\n",
      "+- Project [pais#244, monto#243]\n",
      "   +- Filter (isnotnull(monto#243) AND (monto#243 > 100.0))\n",
      "      +- Relation [id_venta#241,fecha#242,monto#243,pais#244] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[pais#244], functions=[sum(monto#243)], output=[pais#244, sum(monto)#261])\n",
      "   +- Exchange hashpartitioning(pais#244, 12), ENSURE_REQUIREMENTS, [plan_id=406]\n",
      "      +- HashAggregate(keys=[pais#244], functions=[partial_sum(monto#243)], output=[pais#244, sum#265])\n",
      "         +- Filter (isnotnull(monto#243) AND (monto#243 > 100.0))\n",
      "            +- FileScan csv [monto#243,pais#244] Batched: false, DataFilters: [isnotnull(monto#243), (monto#243 > 100.0)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/User_Kenny/Documents/_KENNEDY_MOLINA_PROYECTOS/pyspark-..., PartitionFilters: [], PushedFilters: [IsNotNull(monto), GreaterThan(monto,100.0)], ReadSchema: struct<monto:double,pais:string>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T21:27:32.274612Z",
     "start_time": "2026-02-02T21:27:32.266647Z"
    }
   },
   "cell_type": "code",
   "source": "df_group.explain(\"formatted\")\n",
   "id": "ad0eea2ae4b648a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (6)\n",
      "+- HashAggregate (5)\n",
      "   +- Exchange (4)\n",
      "      +- HashAggregate (3)\n",
      "         +- Filter (2)\n",
      "            +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [monto#243, pais#244]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/C:/Users/User_Kenny/Documents/_KENNEDY_MOLINA_PROYECTOS/pyspark-advanced-databricks/01_data_modeling/src/ventas.csv]\n",
      "PushedFilters: [IsNotNull(monto), GreaterThan(monto,100.0)]\n",
      "ReadSchema: struct<monto:double,pais:string>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [monto#243, pais#244]\n",
      "Condition : (isnotnull(monto#243) AND (monto#243 > 100.0))\n",
      "\n",
      "(3) HashAggregate\n",
      "Input [2]: [monto#243, pais#244]\n",
      "Keys [1]: [pais#244]\n",
      "Functions [1]: [partial_sum(monto#243)]\n",
      "Aggregate Attributes [1]: [sum#264]\n",
      "Results [2]: [pais#244, sum#265]\n",
      "\n",
      "(4) Exchange\n",
      "Input [2]: [pais#244, sum#265]\n",
      "Arguments: hashpartitioning(pais#244, 12), ENSURE_REQUIREMENTS, [plan_id=406]\n",
      "\n",
      "(5) HashAggregate\n",
      "Input [2]: [pais#244, sum#265]\n",
      "Keys [1]: [pais#244]\n",
      "Functions [1]: [sum(monto#243)]\n",
      "Aggregate Attributes [1]: [sum(monto#243)#260]\n",
      "Results [2]: [pais#244, sum(monto#243)#260 AS sum(monto)#261]\n",
      "\n",
      "(6) AdaptiveSparkPlan\n",
      "Output [2]: [pais#244, sum(monto)#261]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
